{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "# !pip install langdetect\n",
    "import string\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "\n",
    "# !pip install -U gensim --user\n",
    "import gensim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random, time\n",
    "import gzip, os, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check if there is any missing value in the dataset ###\n",
    "def check_missing(df, col):\n",
    "    missing  = 0\n",
    "    misVariables = []\n",
    "    CheckNull = df.isnull().sum()\n",
    "    for var in range(0, len(CheckNull)):\n",
    "#         if CheckNull[var] != 0:\n",
    "        misVariables.append([col[var], CheckNull[var], round(CheckNull[var]/len(df),3)])\n",
    "        missing = missing + 1\n",
    "\n",
    "    if missing == 0:\n",
    "        print('Dataset is complete with no blanks.')\n",
    "    else:\n",
    "        print('Totally, %d features have missing values (blanks).' %missing)\n",
    "        df_misVariables = pd.DataFrame.from_records(misVariables)\n",
    "        df_misVariables.columns = ['Variable', 'Missing', 'Percentage (%)']\n",
    "        s = df_misVariables.sort_values(by=['Percentage (%)'], ascending=False).style.bar(subset=['Percentage (%)'], color='#d65f5f')\n",
    "        display(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_info_csv(df_pub):\n",
    "    \n",
    "    ## Extract features we need from the original dataset ### \n",
    "    shortInfo_pub_df = df_pub[['Title', 'Source title', 'Publisher', 'Abstract','DOI', \\\n",
    "                               'Publication Type', 'Dimensions URL', 'Publication Date']]\n",
    "#     shortInfo_pub_df['DOI'] = 'https://doi.org/' + df_pub['DOI']\n",
    "\n",
    "    ### Check the paper langauge ###\n",
    "    lang_paper = []\n",
    "    for i in shortInfo_pub_df['Title']:\n",
    "        try:\n",
    "            lang_paper.append(detect(i))\n",
    "        except:\n",
    "            lang_paper.append(None)\n",
    "\n",
    "    shortInfo_pub_df['Language'] = lang_paper\n",
    "    \n",
    "    ### standardize the name of source title and publisher ###\n",
    "    new_source_title = []\n",
    "    new_publisher = []\n",
    "\n",
    "    for item in pub_df_2['Source title']:\n",
    "        try:\n",
    "            new_source_title.append(item.translate(str.maketrans('', '', string.punctuation)).replace(' ','_').lower())\n",
    "        except:\n",
    "            new_source_title.append(None)\n",
    "    for item in pub_df_2['Publisher']:\n",
    "        try:\n",
    "            new_publisher.append(item.translate(str.maketrans('', '', string.punctuation)).replace(' ','_').lower())\n",
    "        except:\n",
    "            new_publisher.append(None)\n",
    "\n",
    "    ### Save to a new data file ###\n",
    "    shortInfo_pub_df.to_csv('shortInfoPub.csv', index=None)\n",
    "\n",
    "    return shortInfo_pub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_paper_csv(df_pub):\n",
    "\n",
    "    ### Get authors list ###\n",
    "    author_list = []\n",
    "    unique_author_list = []\n",
    "\n",
    "    for author in df_pub['Authors']:\n",
    "        if type(author) == str:\n",
    "            author = author.replace(' ','')\n",
    "            splited_author = author.split(';')\n",
    "            author_list.append(splited_author)\n",
    "            for item in splited_author:\n",
    "                if item not in unique_author_list:\n",
    "                    unique_author_list.append(item)\n",
    "        else:\n",
    "            author_list.append([])\n",
    "\n",
    "\n",
    "    ### Remove unmeaningful author name ###\n",
    "    remove_name = [',', 'UN,']\n",
    "    for each_name in remove_name:\n",
    "        unique_author_list.remove(each_name)\n",
    "\n",
    "\n",
    "    ### Create Author-paper list ###\n",
    "    papers_each_author = []\n",
    "    for unique_author in unique_author_list:\n",
    "        for each_paper in range(0, len(author_list)):\n",
    "            if unique_author in author_list[each_paper]:\n",
    "                papers_each_author.append([unique_author, df_pub['Dimensions URL'][each_paper]])\n",
    "\n",
    "    papers_each_author_df = pd.DataFrame.from_records(papers_each_author)\n",
    "    papers_each_author_df.columns = ['Author', 'Dimensions URL']\n",
    "    \n",
    "    remove_punc_author = []\n",
    "    for item in au_pub['Author']:\n",
    "        remove_punc_author.append(item.translate(str.maketrans('', '', string.punctuation)).replace('ʼ',''))\n",
    "        \n",
    "    papers_each_author_df['Author_nopunc'] = remove_punc_author\n",
    "    papers_each_author_df.to_csv('AuthorsPub.csv', index=None)\n",
    "    \n",
    "    return papers_each_author_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract key words from title ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user -U nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://medium.com/@gaurav5430/using-nltk-for-lemmatizing-sentences-c1bfff963258\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_extracted_keywords_from_title(shortInfo_pub_df):\n",
    "    \n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    stop_words.extend(['e.g', '’'])\n",
    "\n",
    "    title_filtered_sentence=[]\n",
    "\n",
    "    for item in range(0, len(shortInfo_pub_df['Title'])):\n",
    "        if shortInfo_pub_df['Language'][item] == 'en':\n",
    "            lemmatized_title = lemmatize_sentence(shortInfo_pub_df['Title'][item].lower())\n",
    "            tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+\\$[\\d\\.]+|\\S+')\n",
    "            tokens = tokenizer.tokenize(lemmatized_title)\n",
    "\n",
    "            tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "            each_title = []\n",
    "            for tag in tagged:\n",
    "                if tag[0] not in stop_words and tag[0] not in each_title:\n",
    "                    if ('NN' in tag[1]) or ('VB' in tag[1]) or ('JJ' in tag[1]):\n",
    "                        if (tag[0] not in string.punctuation) and (not tag[0].isdigit()) and (len(tag[0])>2):\n",
    "                            each_title.append(tag[0])\n",
    "            title_filtered_sentence.append(each_title)\n",
    "        else:\n",
    "            title_filtered_sentence.append([])\n",
    "    \n",
    "    \n",
    "    keywords_title_paper = []\n",
    "    for each_paper in range(0, len(title_filtered_sentence)):\n",
    "        for each_word in title_filtered_sentence[each_paper]:\n",
    "            keywords_title_paper.append([shortInfo_pub_df['Dimensions URL'][each_paper], each_word])\n",
    "            \n",
    "    return title_filtered_sentence, keywords_title_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use Gensim to find similar keywords ###\n",
    "def similar_keywords(title_filtered_sentence):\n",
    "    \n",
    "    model = gensim.models.Word2Vec(min_count=2, size=700, workers=5)\n",
    "    model.build_vocab(title_filtered_sentence)\n",
    "\n",
    "    corpus_count = model.corpus_count\n",
    "    model.train(title_filtered_sentence, total_examples = corpus_count, epochs = 1000)\n",
    "    \n",
    "    similar_keywords_list = []\n",
    "    for each in range(0, len(keywords_df)):\n",
    "        keyword = keywords_df['Keyword'][each]\n",
    "        try:\n",
    "            similar_keywords = model.wv.most_similar(keyword, topn=5)\n",
    "        except:\n",
    "            similar_keywords = []\n",
    "        for item in similar_keywords:   \n",
    "            similar_keywords_list.append([keywords_df['Dimensions URL'][each], keyword, item[0]])\n",
    "\n",
    "    return similar_keywords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    ### Read dataset from Dimensions ###\n",
    "    df_pub = pd.read_csv('COVID19Publications.csv', sep=';')\n",
    "    col = df_pub.columns\n",
    "    \n",
    "    ## Check missing values in the Dimensions dataset ###\n",
    "#     check_missing(df_pub, col)\n",
    "    \n",
    "    ## Generate a new data file with key features ###\n",
    "    shortInfo_pub_df = paper_info_csv(df_pub)\n",
    "\n",
    "    ## Generate a author-paper data file ###\n",
    "    papers_each_author_df = author_paper_csv\n",
    "\n",
    "    ## Generate a paper-keywords (from title) data file ###\n",
    "    title_filtered_sentence, keywords_title_paper = get_extracted_keywords_from_title(shortInfo_pub_df)\n",
    "            \n",
    "    keywords_df = pd.DataFrame.from_records(keywords_title_paper)\n",
    "    keywords_df.columns = ['Dimensions URL', 'Keyword']\n",
    "    keywords_df.to_csv('extracted_datafiles/keywordsPub.csv', index=None)\n",
    "\n",
    "    ## Get similar keywords and generate new keywords file ###\n",
    "#     similar_keywords_list = similar_keywords(title_filtered_sentence)\n",
    "\n",
    "#     similar_keywords_df = pd.DataFrame.from_records(similar_keywords_list)\n",
    "#     similar_keywords_df.columns = ['Dimensions URL', 'Keyword', 'Similar_Keyword']\n",
    "#     similar_keywords_df.to_csv('similarkeywordsPub.csv',index=None)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortInfo_pub_df = pd.read_csv('extracted_datafiles/shortInfoPub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_filtered_sentence, keywords_title_paper = get_extracted_keywords_from_title(shortInfo_pub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "    keywords_df = pd.DataFrame.from_records(keywords_title_paper)\n",
    "    keywords_df.columns = ['Dimensions URL', 'Keyword']\n",
    "    keywords_df.to_csv('extracted_datafiles/keywordsPub.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDF2Vec training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data structure for knowledge graph\n",
    "def addTriple(net, source, target, edge):\n",
    "    if source in net:\n",
    "        if  target in net[source]:\n",
    "            net[source][target].add(edge)\n",
    "        else:\n",
    "            net[source][target]= set([edge])\n",
    "    else:\n",
    "        net[source]={}\n",
    "        net[source][target] =set([edge])\n",
    "            \n",
    "def getLinks(net, source):\n",
    "    if source not in net:\n",
    "        return {}\n",
    "    return net[source]\n",
    "\n",
    "# Generate paths (entity->relation->entity) by radom walks\n",
    "def randomWalkUniform(triples, startNode, max_depth=5):\n",
    "    next_node =startNode\n",
    "    path = str(startNode)+'->'\n",
    "    for i in range(max_depth):\n",
    "        neighs = getLinks(triples,next_node)\n",
    "        #print (neighs)\n",
    "        if len(neighs) == 0: break\n",
    "        weights = []\n",
    "        queue = []\n",
    "        for neigh in neighs:\n",
    "            for edge in neighs[neigh]:\n",
    "                queue.append((edge,neigh))\n",
    "        edge, next_node = random.choice(queue)\n",
    "        path = path +str(edge)+'->'\n",
    "        path = path +str(next_node)+'->'\n",
    "    path =path.split('->')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the knowledge graph structure\n",
    "def preprocess(fname):\n",
    "    triples = {}\n",
    "\n",
    "    ent_counter = 0\n",
    "    rel_counter = 0\n",
    "    train_counter = 0\n",
    "\n",
    "    print (fname)\n",
    "    #gzfile= gzip.open(fname, mode='rt')\n",
    "\n",
    "    for line in csv.reader(open(fname), delimiter='\\t', quotechar='\"'):\n",
    "        #print (line)\n",
    "        h = line[0]\n",
    "        r = line[1]\n",
    "        t = line[2]\n",
    "        \n",
    "        train_counter +=1\n",
    "\n",
    "        addTriple(triples, h, t, r)\n",
    "        train_counter+=1\n",
    "    print ('Triple:',train_counter)\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query-result.tsv\n",
      "Triple: 342982\n",
      "12144\n"
     ]
    }
   ],
   "source": [
    "file = 'query-result.tsv'\n",
    "triples = preprocess(file)\n",
    "\n",
    "entities = list(triples.keys())\n",
    "vocabulary = entities\n",
    "print (len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do random walks on the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomNWalkUniform(triples, n, walks, path_depth):\n",
    "    path=[]\n",
    "    for k in range(walks):\n",
    "        walk = randomWalkUniform(triples, n, path_depth)\n",
    "        path.append(walk)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to generate features: 00:00:13\n"
     ]
    }
   ],
   "source": [
    "walks = 100\n",
    "path_depth = 10\n",
    "\n",
    "start_time =time.time()\n",
    "sentences =[]\n",
    "for word in vocabulary:\n",
    "    sentences.extend( randomNWalkUniform(triples, word, walks, path_depth) )\n",
    "elapsed_time = time.time() - start_time\n",
    "print ('Time elapsed to generate features:',time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = gensim.models.Word2Vec(size=300, workers=5, window=5, sg=1)\n",
    "model1.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252631606, 488685800)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_count = model1.corpus_count\n",
    "model1.train(sentences, total_examples = corpus_count, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1.wv.most_similar('<https://app.dimensions.ai/details/publication/pub.1126620775>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
